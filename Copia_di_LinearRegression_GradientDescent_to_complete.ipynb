{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giovaberta/App_Sport/blob/main/Copia_di_LinearRegression_GradientDescent_to_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ba5865d",
      "metadata": {
        "id": "3ba5865d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88280a32",
      "metadata": {
        "id": "88280a32"
      },
      "source": [
        "## **Single variable Linear Regression with random generated Training Dataset**\n",
        "### **Gradient Descent**  \n",
        "\n",
        "1. **Training Set creation**  \n",
        "Generate an array with shape `(500,)` of **features** values $x$ in range (0,20), then generate a corresponding array of **labels** $y$ based on $y = 5x - 2$ formula. Add some noise to each $y$ value to spread data and finally plot data to visualize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e63cbd",
      "metadata": {
        "scrolled": true,
        "id": "e3e63cbd"
      },
      "outputs": [],
      "source": [
        "rnstate = np.random.RandomState(1)\n",
        "\n",
        "''' add your code here '''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Gradient Descent algorithm implementation**  \n",
        "Implement GD algorithm:  \n",
        "2.1. initialize parameters $\\theta_0$ and $\\theta_1$ to some value (for example, 0);  \n",
        "2.2. apply model $h_\\theta(x)$ and calculate $y_{predicted} = h_\\theta(x) = \\theta_0 + \\theta_1 x$;  \n",
        "2.3. update $\\theta$ parameters using the GD formulas:  \n",
        "$\\theta_0 = \\theta_0 - \\alpha [-\\frac{1}{m} \\displaystyle\\sum_{i=1}^{m} (y^{(i)} -y_{predicted}^{(i)})]$  \n",
        "$\\theta_1 = \\theta_1 - \\alpha [-\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m} (y^{(i)} -y_{predicted}^{(i)})x^{(i)}]$;  \n",
        "2.4. repeat points 2.2 and 2.3 for a suitable number of iterations;"
      ],
      "metadata": {
        "id": "-XgECNP_ulZw"
      },
      "id": "-XgECNP_ulZw"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zIvnvgbYEUz5"
      },
      "id": "zIvnvgbYEUz5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6169867",
      "metadata": {
        "id": "d6169867"
      },
      "outputs": [],
      "source": [
        "#Gradient Descent\n",
        "\n",
        "m = y.shape[0]        #number of training samples\n",
        "\n",
        "theta_0 = 0           #initial values for theta parameters\n",
        "theta_1 = 0           # in the h = theta_0 + theta_1*x model\n",
        "\n",
        "n_iterations = 1000   #number of GD cycles\n",
        "alpha = 0.01          #learning-rate\n",
        "\n",
        "'''add your code here'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Final model evaluation**  \n",
        "Plot the training set data vs the data calculated using the model trained by GD algorithm. Also show the main model metrics: **mean absolute** error, **mean squared** error and **root mean squared** error."
      ],
      "metadata": {
        "id": "hglacxqu2HWo"
      },
      "id": "hglacxqu2HWo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10cc5fcb",
      "metadata": {
        "id": "10cc5fcb"
      },
      "outputs": [],
      "source": [
        "'''add your code here'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Learning curve**  \n",
        "Add cost function evaluation to each GD step then plot cost values vs the iteration count. How fast does your model converges?"
      ],
      "metadata": {
        "id": "HpCk1MwI4p8u"
      },
      "id": "HpCk1MwI4p8u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b55a46dd",
      "metadata": {
        "id": "b55a46dd"
      },
      "outputs": [],
      "source": [
        "'''add your code here'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edf336d4",
      "metadata": {
        "id": "edf336d4"
      },
      "source": [
        "### **Linear Regression model**\n",
        "Now, you can try the `LinearRegression` model optimizer from `scikit-learn`: this model implement the ordinary least squares linear regression optimization.  \n",
        "\n",
        "Instantiate the `LinearRegression` class, fit the model using the same Training set you used with GD and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e19c92c3",
      "metadata": {
        "id": "e19c92c3"
      },
      "outputs": [],
      "source": [
        "'''add your code here'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72034cec",
      "metadata": {
        "id": "72034cec"
      },
      "source": [
        "### ***SGD Regression model**  \n",
        "Finally you can also try the `SDGRegressor` from `scikit-learn`, which implement a Stochastic GD algorithm.  \n",
        "\n",
        "Instantiate the `SGDRegressor` class, fit the model using the same Training set you used with GD and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec55ab9",
      "metadata": {
        "id": "2ec55ab9"
      },
      "outputs": [],
      "source": [
        "'''add your code here'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d76bdddc",
      "metadata": {
        "id": "d76bdddc"
      },
      "source": [
        "## **Multivariate Linear Regression with random generated Training Dataset**\n",
        "### **Gradient Descent**\n",
        "\n",
        "1. **Training set creation**  \n",
        "This time you have to generate two array of features $x_1$ and $x_2$ and a corresponding array of labels $y$, using some linear formula. Add some noise to each $y$ value to spread data. Also plot your data so that you can visualize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a311730",
      "metadata": {
        "id": "7a311730"
      },
      "outputs": [],
      "source": [
        "rnstate = np.random.RandomState(1)\n",
        "'''add your code here'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, prepare the **feature matrix**:  \n",
        "$X = \\begin{bmatrix}\n",
        "x_0^{(1)} & x_1^{(1)} & x_2^{(1)} \\\\\n",
        "x_0^{(2)} & x_1^{(2)} & x_2^{(2)} \\\\\n",
        "... \\\\\n",
        "x_0^{(m)} & x_1^{(m)} & x_2^{(m)} \\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "where the $x_0$ column is made by values that all equal 1.\n"
      ],
      "metadata": {
        "id": "KaN0Pb_JAjL0"
      },
      "id": "KaN0Pb_JAjL0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5140f16c",
      "metadata": {
        "id": "5140f16c"
      },
      "outputs": [],
      "source": [
        "'''add your code here'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Gradient Descent**  \n",
        "Now you have to implement the vectorized version of the GD algorithm:  \n",
        "2.1. initialize a vector of $\\theta$ parameters $\\Theta = \\begin{bmatrix} \\theta_0 & \\theta_1 & \\theta_2 \\\\ \\end{bmatrix}$;  \n",
        "2.2. apply model $h_\\theta(X)$ and calculate $y_{predicted} = h_\\theta(X) = (X @ \\Theta^T)^T$. Please note that the external transposition operator is needed to transform $y_{predicted}$ into an horizontal array;  \n",
        "2.3. update $\\Theta$ using the GD vectorized formula: $\\Theta = \\Theta - \\alpha [-\\frac{1}{m}(y - y_{predicted})@X]$;  \n",
        "2.4. repeat points 2.2 and 2.3 for a suitable number of iterations."
      ],
      "metadata": {
        "id": "8l-DwOOiCyVx"
      },
      "id": "8l-DwOOiCyVx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b229fb8",
      "metadata": {
        "id": "0b229fb8"
      },
      "outputs": [],
      "source": [
        "#Gradient Descent\n",
        "m = y.shape[0]        #number of training samples\n",
        "\n",
        "n_iterations = 1000   #number of GD cycles\n",
        "alpha = 0.01          #learning-rate\n",
        "'''add your code here'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Final and further evaluations**  \n",
        "Plot your model predicted values and compare them with the true $y$ values, then compute the model metrics. You can finally try comparison with the `scikit-learn` models."
      ],
      "metadata": {
        "id": "kQ5-ND30IhDv"
      },
      "id": "kQ5-ND30IhDv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c90749e",
      "metadata": {
        "scrolled": true,
        "id": "8c90749e"
      },
      "outputs": [],
      "source": [
        "'''add your code here'''\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ML-venv",
      "language": "python",
      "name": "mlvenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}